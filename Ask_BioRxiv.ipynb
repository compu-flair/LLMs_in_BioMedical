{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwhTV2SgwHCw"
   },
   "outputs": [],
   "source": [
    "# installs\n",
    "#pip install langchain tiktoken openai langchainhub chromadb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1704848233823,
     "user": {
      "displayName": "Ahmad Borzou",
      "userId": "12597026365140090461"
     },
     "user_tz": 360
    },
    "id": "r-H68C-R2sBn"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8617,
     "status": "ok",
     "timestamp": 1704853057961,
     "user": {
      "displayName": "Ahmad Borzou",
      "userId": "12597026365140090461"
     },
     "user_tz": 360
    },
    "id": "6n9sDGHP8xrR",
    "outputId": "c4f6c093-edb5-4c50-fc7c-9648fea7fbfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "## change the\n",
    "query    = \"single cell RNA sequencing\"\n",
    "print(type(query))\n",
    "\n",
    "start_date = \"2010-01-01\" ## \"YYYY-MM-DD\"\n",
    "end_date   = \"2023-12-30\"\n",
    "\n",
    "# Replace with the actual bioRxiv API endpoint for searching articles\n",
    "url = f\"https://api.biorxiv.org/details/biorxiv/{start_date}/{end_date}\"\n",
    "\n",
    "# Parameters for the API request\n",
    "params = {\n",
    "    'q': query,  # your search query\n",
    "    'num_results': 100  # number of results to return\n",
    "}\n",
    "\n",
    "# Send a request to the bioRxiv API\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response JSON\n",
    "    number_of_articles = 100  # Filter the first 10 articles\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract article information\n",
    "    articles = data['collection']  # Adjust this based on actual response structure\n",
    "    articles = articles[:number_of_articles]\n",
    "\n",
    "    # Create a directory for articles if it doesn't exist\n",
    "    if not os.path.exists('articles'):\n",
    "        os.makedirs('articles')\n",
    "\n",
    "    for article in articles:\n",
    "        #print(f\"Title: {article['title']}\")\n",
    "\n",
    "        if article['published'] == \"NA\":\n",
    "            continue\n",
    "            \n",
    "        print(f\"Title: {article['title']}\")\n",
    "\n",
    "        doi = article['doi']\n",
    "        # Replace / with _ in DOI to make it a valid filename\n",
    "        filename = doi.replace('/', '_') + '.txt'\n",
    "        # Include the folder name in the path\n",
    "        filepath = os.path.join('articles', filename)\n",
    "\n",
    "        # bioRxiv metadata\n",
    "        full_text_url = f\"https://www.biorxiv.org/content/{doi}.full\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(full_text_url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                texts = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "                # Writing the texts to a file inside the articles folder\n",
    "                with open(filepath, 'w', encoding='utf-8') as file:\n",
    "                    file.write(texts)\n",
    "                print(f\"Text written to {filepath}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Problem reading the article: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load from a directory\n",
    "loader = DirectoryLoader('./articles/', glob=\"./*.txt\", loader_cls=TextLoader) #- We will use this in case of many articles saved in a directory\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the text into smaller documents \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2833"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see how many smaller documents we have created from the large document\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='their uniqueness in the immunological groups. Mice infection, histology and cytokine detection To assess the susceptibility of C57BL/6 mice to gastrointestinal colonization and/or infection with different S. cerevisiae isolates, C57BL/6 female mice (6-8 weeks old) were maintained under specific pathogen-free conditions at the Animal Facility of University of Perugia (Perugia, Italy). Homozygous Il17a-/-, Ifng-/- and Il10-/- mice, on a C57BL/6 background, were bred under specific pathogen-free conditions ( Fig. 3D and E and Supplemental Fig. 10). Experiments were performed according to the Italian Approved Animal Welfare Assurance A 245/2011-B. For gastrointestinal infection, 10 8 yeast cells were injected intragastrically. Mice were monitored for fungal growth at 3 and 7 days after intragastric inoculation in different organs, such as the esophagus, stomach, ileum, and colon, and for dissemination, liver and kidneys. Colony forming units (log10 CFU) were assessed per organ (Â± SEM), by', metadata={'source': 'articles/10.1101_001891.txt'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets select the 4th document to see how it looks as a small chunk\n",
    "texts[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "#from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings # use openai embeddings instead\n",
    "\n",
    "#from langchain_community.vectorstores import Chroma  # We will use this as our local vectorstore\n",
    "\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('openai_api_key')\n",
    "\n",
    "\n",
    "# Embed and store the texts\n",
    "\n",
    "\n",
    "## here we are using OpenAI embeddings but we could also use the sentensetransformer embeddings\n",
    "embedding_function = OpenAIEmbeddings(model = 'text-embedding-ada-002',openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# create the open-source embedding function \n",
    "#embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(texts, embedding_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making a retriever\n",
    "def retriever(query, num_chunks):\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": num_chunks})\n",
    "    docs = retriever.get_relevant_documents(query) # Here we are filtering documents with similar meaning to the query\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sources(documents):\n",
    "    seen_dois = set()  # A set to keep track of DOIs that have already been printed\n",
    "\n",
    "    for doc in documents:\n",
    "        \n",
    "        # Extract the 'source' from the document's metadata\n",
    "        source = doc.metadata['source']\n",
    "        \n",
    "        # Extract the DOI part from the 'source'\n",
    "        parts = source.split('/')[1].split('.')  # Split into parts by '.' after 'articles/'\n",
    "        doi = '.'.join(parts[:-1])  # Join all parts except the last one ('txt')\n",
    "\n",
    "        # Replace underscore with slash in DOI\n",
    "        doi = doi.replace('_', '/')\n",
    "\n",
    "        # Print the DOI if it hasn't been seen before\n",
    "\n",
    "        if doi not in seen_dois:\n",
    "            print( f'Relevant Docs: Doi: {doi} ')\n",
    "            seen_dois.add(doi)  # Add the DOI to the set of seen DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key here\n",
    "OPENAI_API_KEY = os.getenv('openai_api_key')\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def answer_question(content, question):\n",
    "    # Construct the conversation\n",
    "    messages = [{\"role\": \"system\",\n",
    "                 \"content\": \"\"\"\n",
    "                 You are a helpful research assistant. \n",
    "                 You will be given a Content and a Question. Answer the Question by rephrasing and smoothening the Content. \n",
    "                 Do not add any information but what is in the documents. \n",
    "                 If your output do not meaningfully answer the question, say irrelevant documents.\n",
    "                 \"\"\"\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": content},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "               ]\n",
    "\n",
    "\n",
    "    # Make the API call\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\", #\"gpt-4-1106-preview\"\n",
    "            messages=messages,\n",
    "            temperature=1  # creativity\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(query):\n",
    "    docs = retriever(query, num_chunks=5)\n",
    "    content = \"Content: \"\n",
    "    for doc in docs:\n",
    "        content += doc.page_content if hasattr(doc, 'page_content') else ''\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The process of enzymatic pre-amplification in single-cell RNA-Seq lowers the data quality and results in unavoidable loss of sequence information due to significant truncation of the 5â region of the transcript. Additionally, the small amount of mRNA present in a single cell makes the number of obtainable reads per cell much smaller than that obtainable from bulk samples, making rare transcripts harder to detect. These challenges present hurdles in single-cell RNA-Seq, impacting the overall effectiveness of the sequencing platforms.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asking a new question    \n",
    "subject =  \"enzymatic pre-amplification\"\n",
    "question = f\"Question: what do you know about {subject}?\"\n",
    "content = get_content(query)\n",
    "answer_question(content, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Irrelevant documents.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asking a new question    \n",
    "subject =  \"airplane\"\n",
    "question = f\"Question: what do you know about {subject}?\"\n",
    "content = get_content(query)\n",
    "answer_question(content, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM8RytwSYjFeRPjar3epypy",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
